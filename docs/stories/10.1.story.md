# Story 10.1: LLM Provider Abstraction Layer

## Status
Done

## Story
**As a** system developer,
**I want** an abstracted LLM service layer with multiple provider support,
**so that** the AI processing pipeline can use different LLM providers interchangeably with automatic fallback capabilities.

## Acceptance Criteria
1. Abstract interface for LLM providers (protocol/base class with standard methods)
2. DeepSeek provider implementation (text completion, chat)
3. Gemini provider implementation (text completion, chat, vision/image analysis)
4. Environment-based configuration (provider selection, API keys, model settings)
5. Automatic fallback on provider failure (primary → fallback with configurable retry)
6. Usage/cost tracking (token counts, request logging, cost estimation)

## Tasks / Subtasks

- [x] Task 1: Create LLM provider base interface (AC: 1)
  - [x] Create `apps/api/app/services/llm/__init__.py` module
  - [x] Define `LLMProvider` Protocol/ABC with standard methods: `complete()`, `chat()`, `complete_with_vision()`
  - [x] Define request/response models: `LLMRequest`, `LLMResponse`, `LLMMessage`, `LLMUsage`
  - [x] Define provider-specific exceptions: `LLMProviderError`, `LLMRateLimitError`, `LLMAuthError`

- [x] Task 2: Implement DeepSeek provider (AC: 2)
  - [x] Create `apps/api/app/services/llm/deepseek.py`
  - [x] Implement `DeepSeekProvider` class conforming to `LLMProvider` interface
  - [x] Implement `complete()` method for text completion
  - [x] Implement `chat()` method for chat completions
  - [x] Handle API authentication, rate limits, and errors
  - [x] Parse and return standardized `LLMResponse` with usage stats

- [x] Task 3: Implement Gemini provider with vision support (AC: 3)
  - [x] Create `apps/api/app/services/llm/gemini.py`
  - [x] Implement `GeminiProvider` class conforming to `LLMProvider` interface
  - [x] Implement `complete()` and `chat()` methods
  - [x] Implement `complete_with_vision()` for image analysis (OCR, image understanding)
  - [x] Handle multimodal inputs (text + images)
  - [x] Handle API authentication and errors

- [x] Task 4: Add environment configuration (AC: 4)
  - [x] Add LLM settings to `apps/api/app/core/config.py`
  - [x] Add `DCS_DEEPSEEK_API_KEY`, `DCS_GEMINI_API_KEY` environment variables
  - [x] Add `DCS_LLM_PRIMARY_PROVIDER`, `DCS_LLM_FALLBACK_PROVIDER` settings
  - [x] Add model configuration: `DCS_LLM_DEFAULT_MODEL`, `DCS_LLM_MAX_TOKENS`
  - [x] Add timeout and retry settings

- [x] Task 5: Implement LLM service with fallback logic (AC: 5)
  - [x] Create `apps/api/app/services/llm/service.py`
  - [x] Implement `LLMService` class that wraps providers
  - [x] Implement provider factory method based on configuration
  - [x] Implement automatic fallback: try primary, on failure try fallback provider
  - [x] Implement configurable retry logic with exponential backoff
  - [x] Log provider switches and failures

- [x] Task 6: Implement usage tracking (AC: 6)
  - [x] Create usage tracking data structures in `LLMResponse`
  - [x] Track: prompt tokens, completion tokens, total tokens, estimated cost
  - [x] Implement cost estimation per provider (configurable rates)
  - [x] Add logging for usage statistics
  - [x] Consider future: database persistence for usage analytics

- [x] Task 7: Write unit tests
  - [x] Create `apps/api/tests/test_llm_service.py`
  - [x] Test `LLMProvider` interface contracts
  - [x] Test DeepSeek provider with mocked API responses
  - [x] Test Gemini provider with mocked API responses (including vision)
  - [x] Test fallback logic (primary failure → fallback success)
  - [x] Test configuration loading
  - [x] Test usage tracking calculations

## Dev Notes

### Project Structure
New files to create under `apps/api/app/services/llm/`:
```
apps/api/app/services/llm/
├── __init__.py          # Module exports
├── base.py              # LLMProvider protocol, models, exceptions
├── deepseek.py          # DeepSeek provider implementation
├── gemini.py            # Gemini provider implementation (with vision)
└── service.py           # LLMService with fallback logic
```

### Service Pattern Reference
Follow existing service patterns from `apps/api/app/services/webhook.py`:
- Class-based services with dependency injection
- Async methods for HTTP calls (use `httpx.AsyncClient`)
- Proper logging with `logging.getLogger(__name__)`
- Type hints throughout

### Configuration Pattern Reference
Follow `apps/api/app/core/config.py` Settings pattern:
- Use pydantic `BaseSettings` with `DCS_` prefix
- Add new settings as class attributes with defaults
- Use `@property` for computed values

### Environment Variables (from Epic)
```bash
# LLM Providers
DCS_DEEPSEEK_API_KEY=sk-xxx
DCS_GEMINI_API_KEY=xxx

# Provider Selection
DCS_LLM_PRIMARY_PROVIDER=deepseek
DCS_LLM_FALLBACK_PROVIDER=gemini

# Model Configuration
DCS_LLM_DEFAULT_MODEL=deepseek-chat
DCS_LLM_MAX_TOKENS=4096
DCS_LLM_TIMEOUT_SECONDS=60
```

### API Integration Notes
**DeepSeek API:**
- Base URL: `https://api.deepseek.com/v1`
- OpenAI-compatible API format
- Auth: Bearer token in Authorization header
- Models: `deepseek-chat`, `deepseek-coder`

**Gemini API:**
- Use `google-generativeai` Python SDK
- Models: `gemini-pro`, `gemini-pro-vision`
- Vision: Pass images as base64 or file paths
- Auth: API key in client initialization

### Data Models (to implement)
```python
# Request/Response models
@dataclass
class LLMMessage:
    role: str  # "system", "user", "assistant"
    content: str
    images: list[bytes] | None = None  # For vision requests

@dataclass
class LLMRequest:
    messages: list[LLMMessage]
    model: str | None = None
    max_tokens: int | None = None
    temperature: float = 0.7

@dataclass
class LLMUsage:
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int
    estimated_cost_usd: float | None = None

@dataclass
class LLMResponse:
    content: str
    usage: LLMUsage
    model: str
    provider: str
    raw_response: dict | None = None
```

### Technical Notes from Epic
- Shared design with LMS Epic 13 (can be extracted to shared package later)
- Focus on clean abstraction - other stories will consume this service

## Testing

### Test Location
`apps/api/tests/test_llm_service.py`

### Testing Standards
- Use pytest with async support (`pytest-asyncio`)
- Mock external API calls (never call real APIs in tests)
- Use `unittest.mock` or `pytest-mock` for mocking
- Test both success and failure paths
- Test fallback behavior explicitly

### Test Cases to Cover
1. Provider interface contract tests
2. DeepSeek completion/chat (mocked)
3. Gemini completion/chat/vision (mocked)
4. Fallback: primary fails → fallback succeeds
5. Fallback: both providers fail → raises exception
6. Configuration loading and validation
7. Usage tracking token counting
8. Cost estimation calculations

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2024-12-30 | 0.1 | Initial story draft | Bob (SM Agent) |

## Dev Agent Record

### Agent Model Used
Claude Opus 4.5 (claude-opus-4-5-20251101)

### Debug Log References
None - implementation proceeded without blocking issues.

### Completion Notes List
- Implemented LLM provider abstraction with Protocol-based interface
- DeepSeek provider uses OpenAI-compatible API format
- Gemini provider includes full vision support with image type detection
- LLMService provides automatic fallback with exponential backoff retry
- Cost estimation implemented per provider with configurable pricing tables
- All 32 unit tests passing with comprehensive mocking of external APIs
- Pre-existing test failures in other modules unrelated to this story

### File List
**Created:**
- `apps/api/app/services/llm/__init__.py` - Module exports
- `apps/api/app/services/llm/base.py` - LLMProvider protocol, models, exceptions
- `apps/api/app/services/llm/deepseek.py` - DeepSeek provider implementation
- `apps/api/app/services/llm/gemini.py` - Gemini provider with vision support
- `apps/api/app/services/llm/service.py` - LLMService with fallback logic
- `apps/api/tests/test_llm_service.py` - Unit tests (32 tests)

**Modified:**
- `apps/api/app/core/config.py` - Added LLM configuration settings

## QA Results

### Review Date: 2024-12-30

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment

**Overall: Excellent** - Clean, well-structured implementation following established project patterns.

**Strengths:**
- Well-designed abstraction using ABC pattern for provider interface
- Proper async patterns with httpx for HTTP calls
- Comprehensive error hierarchy with specific exception types
- Type hints throughout with proper dataclass usage
- Good separation of concerns (base models, providers, service layer)
- Logging at appropriate levels (debug for requests, info for completions)

**Architecture:**
- Provider pattern allows easy addition of new LLM providers
- Fallback mechanism with exponential backoff is production-ready
- Configuration follows existing pydantic Settings pattern

### Refactoring Performed

None required - implementation is clean and follows project standards.

### Compliance Check

- Coding Standards: ✓ Follows existing service patterns (webhook.py reference)
- Project Structure: ✓ Files correctly placed in `services/llm/`
- Testing Strategy: ✓ Pytest with async support, proper mocking
- All ACs Met: ✓ All 6 acceptance criteria fully implemented

### Requirements Traceability

| AC | Implementation | Test Coverage |
|----|----------------|---------------|
| 1. Abstract interface | `base.py:LLMProvider` ABC with 4 abstract methods | `TestLLMMessage`, `TestLLMRequest`, `TestLLMUsage` |
| 2. DeepSeek provider | `deepseek.py:DeepSeekProvider` with complete/chat | `TestDeepSeekProvider` (6 tests) |
| 3. Gemini provider | `gemini.py:GeminiProvider` with vision support | `TestGeminiProvider` (4 tests) |
| 4. Environment config | `config.py` with 8 LLM settings | `test_no_providers_configured` |
| 5. Automatic fallback | `service.py:LLMService._execute_with_retry` | `TestLLMService` (8 tests) |
| 6. Usage tracking | `LLMUsage` dataclass + `estimate_cost()` | `test_cost_estimation` tests |

### Improvements Checklist

- [x] All acceptance criteria implemented
- [x] Comprehensive unit tests (32 tests, all passing)
- [x] Proper error handling for auth, rate limits, connection errors
- [x] Cost estimation for both providers
- [x] Vision support implemented for Gemini
- [ ] Consider: Move pricing data to configuration (minor - future enhancement)
- [ ] Consider: Add usage persistence to database (noted in story for future)
- [ ] Consider: Add circuit breaker pattern for provider failures (nice-to-have)

### Security Review

✓ **PASS** - No security concerns identified.

- API keys loaded from environment variables (not hardcoded)
- No sensitive data logged (keys not exposed in logs)
- Proper error handling doesn't leak sensitive details
- HTTPS used for all provider API calls

### Performance Considerations

✓ **PASS** - No performance concerns identified.

- Async HTTP calls with configurable timeouts
- Connection reuse within request context (httpx.AsyncClient)
- Exponential backoff prevents thundering herd on rate limits
- Cost estimation is O(1) calculation

### Test Architecture Assessment

**Test Coverage: Comprehensive**

| Category | Tests | Coverage |
|----------|-------|----------|
| Base Models | 10 | LLMMessage, LLMRequest, LLMUsage validation |
| DeepSeek Provider | 6 | Success, chat, vision-not-supported, cost, auth, rate-limit |
| Gemini Provider | 4 | Success, vision, image-type-detection, cost |
| LLM Service | 8 | Primary success, fallback, both-fail, no-fallback, force-provider, simple-completion, vision, no-config |
| Exceptions | 4 | All exception types |
| **Total** | **32** | All paths covered |

**Test Quality:**
- Proper mocking of external API calls
- Tests both success and failure scenarios
- Edge cases covered (rate limits, auth errors, no providers)
- Async tests using pytest-asyncio

### Files Modified During Review

None - no modifications required.

### Gate Status

**Gate: PASS** → `docs/qa/gates/10.1-llm-provider-abstraction.yml`

### Recommended Status

**✓ Ready for Done** - All acceptance criteria met, comprehensive test coverage, clean implementation following project patterns. No blocking issues identified.
