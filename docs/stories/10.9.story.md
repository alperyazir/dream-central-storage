# Story 10.9: AI Data Storage Structure

## Status
Done

## Story
**As a** system developer,
**I want** a unified AI data storage service that manages the consolidated metadata.json and directory structure,
**so that** the AI processing pipeline has a consistent storage interface with proper initialization, atomic writes, and cleanup capabilities.

## Acceptance Criteria
1. Create `/ai-data/` directory structure on processing start
2. Store text files per page
3. Store module JSON files
4. Store vocabulary.json
5. Store audio files organized by language
6. Store metadata.json with processing info
7. Atomic writes (prevent partial data)
8. Cleanup on re-processing

## Tasks / Subtasks

- [x] Task 1: Create unified AI data metadata service module structure (AC: 1, 6-8)
  - [x] Create `apps/api/app/services/ai_data/__init__.py` module
  - [x] Create `apps/api/app/services/ai_data/models.py` with dataclasses
  - [x] Define `ProcessingMetadata` dataclass matching metadata.json schema
  - [x] Define `AIDataStructure` dataclass for directory paths tracking
  - [x] Define exceptions: `AIDataStorageError`, `MetadataError`, `InitializationError`

- [x] Task 2: Create metadata service (AC: 6-7)
  - [x] Create `apps/api/app/services/ai_data/service.py`
  - [x] Implement `AIDataMetadataService` class
  - [x] Implement `create_metadata()` - create initial metadata.json with processing_status="processing"
  - [x] Implement `update_metadata()` - update metadata with stage results
  - [x] Implement `finalize_metadata()` - set final status (completed/failed/partial) and timestamps
  - [x] Implement `get_metadata()` - retrieve current metadata.json
  - [x] Implement `get_ai_data_metadata_service()` singleton factory

- [x] Task 3: Create directory structure manager (AC: 1)
  - [x] Create `apps/api/app/services/ai_data/structure.py`
  - [x] Implement `AIDataStructureManager` class
  - [x] Implement `initialize_ai_data_structure()` - ensure all ai-data subdirectories exist
  - [x] Implement `get_ai_data_paths()` - return all expected paths for ai-data structure
  - [x] Implement `verify_structure()` - check if ai-data structure is properly initialized
  - [x] Implement `get_ai_data_structure_manager()` singleton factory

- [x] Task 4: Create cleanup manager (AC: 8)
  - [x] Create `apps/api/app/services/ai_data/cleanup.py`
  - [x] Implement `AIDataCleanupManager` class
  - [x] Implement `cleanup_all()` - remove all ai-data content for re-processing
  - [x] Implement `cleanup_selective()` - remove specific subdirectories (text, modules, audio)
  - [x] Implement `get_cleanup_stats()` - return count of deleted files per directory
  - [x] Implement `get_ai_data_cleanup_manager()` singleton factory

- [x] Task 5: Integrate with queue processing pipeline (AC: 1, 6-8)
  - [x] Update `apps/api/app/services/queue/tasks.py`
  - [x] Import AI data services in tasks.py
  - [x] Call `initialize_ai_data_structure()` at start of `process_book_task()`
  - [x] Create initial `metadata.json` with status "processing"
  - [x] Update `metadata.json` after each stage completes with stage results
  - [x] Call `finalize_metadata()` at end of processing with aggregated stats
  - [x] Handle cleanup on re-processing when job starts

- [x] Task 6: Write unit tests (AC: 1-8)
  - [x] Create `apps/api/tests/test_ai_data.py`
  - [x] Test metadata.json creation with correct schema
  - [x] Test metadata.json update after each stage
  - [x] Test finalize with completed/partial/failed status
  - [x] Test directory structure initialization
  - [x] Test structure verification
  - [x] Test cleanup all ai-data
  - [x] Test selective cleanup
  - [x] Test error handling for storage failures
  - [x] Mock MinIO for isolated testing

## Dev Notes

### Previous Story Insights (Story 10.8)
- **Pattern Used:** Module structure follows `services/{module}/` pattern with `models.py`, `service.py`, `storage.py`, `__init__.py`
- **Singleton Pattern:** `get_*_service()` factory function for global instance
- **Exception Hierarchy:** Specific exceptions inherit from base error class with book_id context
- **Async Patterns:** Using async/await throughout services (though MinIO operations are sync)
- **Path Building:** `_build_ai_data_path()` helper method for consistent path construction
- **Cleanup Pattern:** Each storage service has `cleanup_*_directory()` method

[Source: Story 10.8 Dev Agent Record]

### Current Storage Implementation Overview

The following storage services already exist and handle their respective data:

| Service | Location | Stores | Metadata File |
|---------|----------|--------|---------------|
| `AIDataStorage` | `pdf/storage.py` | `ai-data/text/page_*.txt` | `extraction_metadata.json` |
| `ModuleStorage` | `segmentation/storage.py` | `ai-data/modules/module_*.json` | `segmentation_metadata.json` |
| `TopicStorage` | `topic_analysis/storage.py` | Updates module JSONs | `topic_analysis_metadata.json` |
| `VocabularyStorage` | `vocabulary_extraction/storage.py` | `ai-data/vocabulary.json` | `vocabulary_metadata.json` |
| `AudioStorage` | `audio_generation/storage.py` | `ai-data/audio/vocabulary/{lang}/*.mp3` | (none) |

**Missing:** A unified `metadata.json` at `ai-data/metadata.json` that aggregates all processing info.

[Source: apps/api/app/services/*/storage.py]

### metadata.json Schema (from Epic 10)

The master `metadata.json` file should aggregate all processing information:

```json
{
  "book_id": "uuid",
  "publisher_id": "uuid",
  "book_name": "book-folder-name",
  "processing_status": "completed",  // pending|processing|completed|partial|failed
  "processing_started_at": "2024-01-15T10:00:00Z",
  "processing_completed_at": "2024-01-15T10:35:00Z",
  "total_pages": 120,
  "total_modules": 8,
  "total_vocabulary": 150,
  "total_audio_files": 300,
  "languages": ["en", "tr"],
  "primary_language": "en",
  "difficulty_range": ["A1", "A2", "B1"],
  "llm_provider": "deepseek",
  "tts_provider": "edge",
  "stages": {
    "text_extraction": {
      "status": "completed",
      "completed_at": "...",
      "total_pages": 120,
      "method": "native"
    },
    "segmentation": {
      "status": "completed",
      "completed_at": "...",
      "module_count": 8
    },
    "topic_analysis": {
      "status": "completed",
      "completed_at": "...",
      "primary_language": "en"
    },
    "vocabulary": {
      "status": "completed",
      "completed_at": "...",
      "total_words": 150
    },
    "audio_generation": {
      "status": "completed",
      "completed_at": "...",
      "audio_files_count": 300
    }
  },
  "errors": []
}
```

[Source: docs/prd/epic-10-ai-book-processing-pipeline.md#data-schemas]

### Storage Structure Reference

```
/publishers/{publisher_id}/books/{book_id}/{book_name}/
└── ai-data/
    ├── metadata.json              <- NEW: This story creates
    ├── text/
    │   ├── page_001.txt
    │   ├── page_002.txt
    │   └── extraction_metadata.json
    ├── modules/
    │   ├── module_1.json
    │   ├── module_2.json
    │   └── segmentation_metadata.json
    ├── vocabulary.json
    ├── vocabulary_metadata.json
    └── audio/
        └── vocabulary/
            ├── en/
            │   └── *.mp3
            └── tr/
                └── *.mp3
```

[Source: docs/prd/epic-10-ai-book-processing-pipeline.md#storage-structure]

### Queue Task Integration Points

The `process_book_task()` function in `tasks.py` orchestrates all stages:

```python
# Current flow (simplified):
async def process_book_task(...):
    # 1. Update job status to PROCESSING
    await repository.update_job_status(job_id, ProcessingStatus.PROCESSING)

    # 2. Run each stage
    for stage in stages_to_run:
        result = await _run_processing_stage(stage, ...)
        stage_results[stage] = result

    # 3. Update final status
    await repository.update_job_status(job_id, ProcessingStatus.COMPLETED)
```

**Integration points for this story:**
1. Before stage loop: Initialize ai-data structure and create initial metadata.json
2. After each stage: Update metadata.json with stage results
3. After loop completes: Finalize metadata.json with totals and final status
4. On re-processing: Call cleanup before initialization

[Source: apps/api/app/services/queue/tasks.py:35-171]

### MinIO Path Building Pattern

All storage services use the same path building pattern:

```python
def _build_ai_data_path(
    self,
    publisher_id: str,
    book_id: str,
    book_name: str,
    *path_parts: str,
) -> str:
    """Build MinIO path within ai-data directory."""
    base = f"{publisher_id}/books/{book_id}/{book_name}/ai-data"
    if path_parts:
        return f"{base}/{'/'.join(path_parts)}"
    return base
```

Use this pattern for `metadata.json` path: `_build_ai_data_path(..., "metadata.json")`

[Source: apps/api/app/services/vocabulary_extraction/storage.py:49-60]

### MinIO Operations

MinIO client operations (sync, not async):

```python
from app.services.minio import get_minio_client
from minio.error import S3Error

client = get_minio_client(settings)
bucket = settings.minio_publishers_bucket

# Write JSON file
json_bytes = json.dumps(data, indent=2).encode("utf-8")
client.put_object(bucket, path, BytesIO(json_bytes), len(json_bytes), content_type="application/json")

# Read JSON file
response = client.get_object(bucket, path)
data = json.loads(response.read().decode("utf-8"))
response.close()
response.release_conn()

# Check if exists
try:
    client.stat_object(bucket, path)
    exists = True
except S3Error as e:
    if e.code == "NoSuchKey":
        exists = False

# List objects
objects = client.list_objects(bucket, prefix=prefix, recursive=True)

# Delete object
client.remove_object(bucket, object_name)
```

[Source: apps/api/app/services/pdf/storage.py]

### Settings Access

```python
from app.core.config import get_settings

settings = get_settings()
settings.minio_publishers_bucket  # Bucket name
settings.llm_primary_provider     # "deepseek"
settings.tts_primary_provider     # "edge"
```

[Source: apps/api/app/core/config.py]

### Processing Status Values

From queue models:

```python
class ProcessingStatus(str, Enum):
    PENDING = "pending"
    QUEUED = "queued"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    PARTIAL = "partial"
```

[Source: apps/api/app/services/queue/models.py]

### Project File Structure

New files to create:
```
apps/api/app/services/ai_data/
├── __init__.py              # Module exports
├── models.py                # ProcessingMetadata, AIDataStructure, exceptions
├── service.py               # AIDataMetadataService
├── structure.py             # AIDataStructureManager
└── cleanup.py               # AIDataCleanupManager
```

[Source: Pattern from apps/api/app/services/audio_generation/]

### Note: ACs 2-5 Already Implemented

Acceptance Criteria 2-5 are already satisfied by existing stories:
- AC2: Text files - Story 10.4 (AIDataStorage)
- AC3: Module JSONs - Story 10.5 (ModuleStorage)
- AC4: vocabulary.json - Story 10.7 (VocabularyStorage)
- AC5: Audio files - Story 10.8 (AudioStorage)

This story focuses on AC1 (structure init), AC6 (metadata.json), AC7 (atomic writes), AC8 (cleanup).

## Testing

### Test Location
`apps/api/tests/test_ai_data.py`

### Testing Standards
- Use pytest with async support (`pytest-asyncio`)
- Mock MinIO storage operations
- Use sample book data fixtures
- Follow patterns from `apps/api/tests/test_audio_generation.py`

[Source: docs/architecture/16-testing-strategy.md]

### Test Cases to Cover
1. ProcessingMetadata dataclass serialization/deserialization
2. Initial metadata.json creation with correct schema
3. Metadata update with stage results (text_extraction, segmentation, etc.)
4. Finalize metadata with completed status
5. Finalize metadata with partial status (some stages failed)
6. Finalize metadata with failed status
7. Directory structure initialization verification
8. Cleanup all ai-data content
9. Selective cleanup (text only, modules only, etc.)
10. Error handling for MinIO failures
11. Get metadata when not exists (returns None)
12. Provider info population from settings

## Dependencies

### Python Packages
No new packages required - uses existing:
- `minio` (for storage)
- `json` (for serialization)
- `dataclasses` (for models)

### Existing Services Used
- `get_minio_client()` from `app.services.minio`
- `get_settings()` from `app.core.config`
- Queue models from `app.services.queue.models`

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2024-12-30 | 0.1 | Initial story draft | Bob (SM Agent) |
| 2024-12-30 | 1.0 | Implementation complete | James (Dev Agent) |

## Dev Agent Record

### Agent Model Used
Claude Opus 4.5 (claude-opus-4-5-20251101)

### Debug Log References
N/A - No debug logs generated

### Completion Notes List
- Created new `ai_data` service module with 4 files following existing patterns
- Implemented `ProcessingMetadata` dataclass matching Epic 10 metadata.json schema
- Implemented `StageResult` dataclass for per-stage tracking
- Implemented `AIDataStructure` dataclass for path management
- Integrated metadata tracking into queue pipeline at 3 points: init, stage complete, finalize
- Added automatic cleanup on re-processing detection
- All 36 unit tests pass
- Linting passes with ruff

### File List
**Created:**
- `apps/api/app/services/ai_data/__init__.py` - Module exports
- `apps/api/app/services/ai_data/models.py` - Data models and exceptions
- `apps/api/app/services/ai_data/service.py` - AIDataMetadataService
- `apps/api/app/services/ai_data/structure.py` - AIDataStructureManager
- `apps/api/app/services/ai_data/cleanup.py` - AIDataCleanupManager
- `apps/api/tests/test_ai_data.py` - 36 unit tests

**Modified:**
- `apps/api/app/services/queue/tasks.py` - Added AI data service integration

## QA Results

### Review Date: 2024-12-30

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment

**Overall: GOOD**

The implementation follows established project patterns consistently:
- Clean module structure (`models.py`, `service.py`, `structure.py`, `cleanup.py`, `__init__.py`)
- Proper exception hierarchy with context (book_id in all exceptions)
- Well-documented code with comprehensive docstrings
- Correct use of dataclasses with `to_dict()`/`from_dict()` serialization
- Singleton pattern via factory functions matches existing services
- Proper error handling for MinIO S3Error cases
- UTF-8 encoding handled correctly in JSON serialization

**Architecture Strengths:**
- Clear separation of concerns (metadata, structure, cleanup)
- Integration points in tasks.py are well-placed (init, stage complete, finalize)
- Automatic cleanup on re-processing prevents stale data
- Aggregated fields update logic is clean and maintainable

### Refactoring Performed

None required - code quality is good.

### Compliance Check

- Coding Standards: ✓ Follows existing patterns, proper typing, docstrings
- Project Structure: ✓ Correct module location under `services/ai_data/`
- Testing Strategy: ✓ Unit tests with mocked MinIO, follows pytest patterns
- All ACs Met: ✓ AC1, AC6, AC7, AC8 implemented; AC2-5 correctly noted as pre-existing

### Improvements Checklist

All items are recommendations for future consideration, not blockers:

- [x] Code structure follows established patterns
- [x] Exception handling is comprehensive
- [x] All public methods have docstrings
- [x] Tests cover happy paths and error scenarios
- [ ] Consider adding integration test for full tasks.py pipeline flow (future story)
- [ ] Consider adding concurrency protection for cleanup (edge case: two jobs for same book)
- [ ] Consider resetting singleton instances in test teardown for better isolation

### Security Review

**Status: PASS**

- No sensitive data exposure in metadata.json
- No hardcoded credentials or secrets
- Proper error handling prevents information leakage
- MinIO credentials accessed via settings (not hardcoded)

### Performance Considerations

**Status: PASS**

- MinIO operations are appropriately synchronous (S3 operations are I/O bound)
- Directory initialization uses efficient stat_object checks before creation
- Cleanup uses list_objects with recursive=True for efficient bulk deletion
- JSON serialization includes `ensure_ascii=False` for efficient Unicode handling

### Files Modified During Review

None - no refactoring performed.

### Gate Status

Gate: **PASS** → docs/qa/gates/10.9-ai-data-storage-structure.yml
Risk profile: LOW (no security/auth changes, good test coverage)
NFR assessment: All PASS

### Recommended Status

**✓ Ready for Done**

The implementation is solid, follows established patterns, and has comprehensive test coverage (36 tests). All acceptance criteria are met. The minor recommendations are for future consideration and do not block the story.
