# Story 10.3: AI Processing Queue System

## Status
Done

## Story
**As a** system developer,
**I want** a background job queue system for AI book processing tasks,
**so that** PDF processing can run asynchronously without blocking uploads, with proper status tracking, progress reporting, and automatic retry capabilities.

## Acceptance Criteria
1. Job queue for processing tasks (using arq with Redis)
2. Processing status tracking (queued, processing, completed, failed, partial)
3. Progress reporting (percentage completion per job)
4. Error handling and retry logic (configurable attempts with exponential backoff)
5. Concurrent processing limits (configurable worker concurrency)
6. Priority queue for re-processing (manual re-trigger with priority)

## Tasks / Subtasks

- [x] Task 1: Create queue infrastructure models (AC: 1, 2)
  - [x] Create `apps/api/app/services/queue/__init__.py` module
  - [x] Define `ProcessingJob` dataclass with fields: job_id, book_id, status, progress, created_at, started_at, completed_at
  - [x] Define `ProcessingStatus` enum: QUEUED, PROCESSING, COMPLETED, FAILED, PARTIAL, CANCELLED
  - [x] Define `ProcessingJobType` enum: FULL, TEXT_ONLY, VOCABULARY_ONLY, AUDIO_ONLY
  - [x] Define queue-specific exceptions: `QueueError`, `JobNotFoundError`, `JobAlreadyExistsError`

- [x] Task 2: Create Redis connection management (AC: 1)
  - [x] Create `apps/api/app/services/queue/redis.py`
  - [x] Implement `RedisConnection` class for connection pooling
  - [x] Implement health check method
  - [x] Handle connection errors gracefully
  - [x] Support both standard Redis and Redis cluster

- [x] Task 3: Add queue configuration settings (AC: 1, 5)
  - [x] Add queue settings to `apps/api/app/core/config.py`
  - [x] Add `DCS_REDIS_URL` environment variable (default: `redis://localhost:6379`)
  - [x] Add `DCS_QUEUE_NAME` setting (default: `ai_processing`)
  - [x] Add `DCS_QUEUE_MAX_CONCURRENCY` setting (default: 3)
  - [x] Add `DCS_QUEUE_MAX_RETRIES` setting (default: 3)
  - [x] Add `DCS_QUEUE_JOB_TIMEOUT_SECONDS` setting (default: 3600 for 1 hour)
  - [x] Add `DCS_QUEUE_RETRY_DELAY_SECONDS` setting (default: 60)

- [x] Task 4: Implement job repository (AC: 2, 3)
  - [x] Create `apps/api/app/services/queue/repository.py`
  - [x] Implement `JobRepository` class for Redis-based job storage
  - [x] Implement `create_job()` - stores job metadata in Redis hash
  - [x] Implement `get_job()` - retrieves job by ID
  - [x] Implement `update_job_status()` - updates status and timestamps
  - [x] Implement `update_job_progress()` - updates progress percentage (0-100)
  - [x] Implement `list_jobs()` - list jobs with filtering by status, book_id
  - [x] Implement `delete_job()` - removes job record
  - [x] Use Redis TTL for automatic cleanup of old completed/failed jobs

- [x] Task 5: Implement queue service with arq (AC: 1, 4, 5, 6)
  - [x] Create `apps/api/app/services/queue/service.py`
  - [x] Install `arq` package (add to pyproject.toml dependencies)
  - [x] Implement `QueueService` class wrapping arq functionality
  - [x] Implement `enqueue_job()` - adds job to queue with priority support
  - [x] Implement `get_job_status()` - retrieves current job state
  - [x] Implement `cancel_job()` - cancels queued/in-progress job
  - [x] Implement `retry_job()` - re-queues failed job with priority
  - [x] Implement `get_queue_stats()` - returns queue depth, active workers, etc.
  - [x] Support job priority (HIGH, NORMAL, LOW)

- [x] Task 6: Create worker task definitions (AC: 1, 4)
  - [x] Create `apps/api/app/services/queue/tasks.py`
  - [x] Define `process_book_task()` - main entry point for book processing
  - [x] Implement task wrapper with progress callback
  - [x] Implement error handling with retry logic
  - [x] Implement partial failure handling (continue on non-critical errors)
  - [x] Add logging for task start, progress, completion, failure

- [x] Task 7: Implement progress reporting (AC: 3)
  - [x] Create `ProgressReporter` class in `service.py`
  - [x] Implement percentage calculation based on processing steps
  - [x] Define processing stages: TEXT_EXTRACTION (20%), SEGMENTATION (15%), TOPIC_ANALYSIS (20%), VOCABULARY (20%), AUDIO (25%)
  - [x] Implement `report_progress()` to update Redis job record
  - [x] Implement `report_step_complete()` for stage transitions
  - [x] Support sub-step progress within stages

- [x] Task 8: Create worker runner script (AC: 1, 5)
  - [x] Create `apps/api/app/services/queue/worker.py`
  - [x] Implement arq `WorkerSettings` class
  - [x] Configure concurrency limits from settings
  - [x] Configure retry behavior with exponential backoff
  - [x] Add graceful shutdown handling
  - [x] Create CLI entry point: `python -m app.services.queue.worker`

- [x] Task 9: Write unit tests (AC: 1-6)
  - [x] Create `apps/api/tests/test_queue_service.py`
  - [x] Test job creation and retrieval
  - [x] Test status transitions (QUEUED → PROCESSING → COMPLETED)
  - [x] Test progress reporting (0% → 50% → 100%)
  - [x] Test retry logic with mocked failures
  - [x] Test priority queue ordering
  - [x] Test concurrent job limits
  - [x] Test job cancellation
  - [x] Test queue statistics

## Dev Notes

### Previous Story Insights (Stories 10.1, 10.2)
- **Pattern Used:** ABC (Abstract Base Class) interface design for providers
- **Service Structure:** `services/{module}/` with `__init__.py`, `base.py`, `{implementation}.py`, `service.py`
- **Async Patterns:** Using `httpx.AsyncClient` for HTTP calls, async/await throughout
- **Error Hierarchy:** Specific exception types inheriting from base error
- **Configuration:** Pydantic `BaseSettings` with `DCS_` prefix
- **Testing:** pytest-asyncio with mocked external dependencies
- **Singleton Pattern:** `get_service()` function for global service instance

[Source: Story 10.1-10.2 Implementation]

### Project Structure
New files to create under `apps/api/app/services/queue/`:
```
apps/api/app/services/queue/
├── __init__.py          # Module exports
├── models.py            # ProcessingJob, ProcessingStatus, enums
├── redis.py             # Redis connection management
├── repository.py        # JobRepository for Redis storage
├── service.py           # QueueService, ProgressReporter
├── tasks.py             # Task definitions for arq worker
└── worker.py            # Worker runner configuration
```

[Source: Pattern from apps/api/app/services/llm/, tts/]

### Queue Technology Selection

**arq (Async Redis Queue) - RECOMMENDED:**
- Native async/await support (matches project patterns)
- Redis-based (reliable, widely deployed)
- Built-in retry with exponential backoff
- Job result storage with TTL
- Lightweight (~500 lines of code)
- Active maintenance, good documentation
- Library: `arq>=0.26,<0.27`

**Why not Celery:**
- Heavier dependency footprint
- Sync-first design (requires workarounds for async)
- More complex configuration

**Why not rq (Redis Queue):**
- Sync-only execution
- Doesn't align with async codebase patterns

[Source: Epic 10 requirements, project async patterns]

### Configuration Pattern Reference
Follow `apps/api/app/core/config.py` Settings pattern:
- Use pydantic `BaseSettings` with `DCS_` prefix
- Add new settings as class attributes with defaults
- Environment variables map automatically (e.g., `DCS_REDIS_URL` → `redis_url`)

[Source: apps/api/app/core/config.py:54-75]

### Environment Variables (from Epic)
```bash
# Redis Configuration
DCS_REDIS_URL=redis://localhost:6379
DCS_QUEUE_NAME=ai_processing

# Processing Configuration
DCS_QUEUE_MAX_CONCURRENCY=3
DCS_QUEUE_MAX_RETRIES=3
DCS_QUEUE_JOB_TIMEOUT_SECONDS=3600
DCS_QUEUE_RETRY_DELAY_SECONDS=60

# Processing Priority
DCS_QUEUE_DEFAULT_PRIORITY=normal
```

[Source: docs/prd/epic-10-ai-book-processing-pipeline.md#environment-variables]

### Data Models (to implement)
```python
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum

class ProcessingStatus(str, Enum):
    QUEUED = "queued"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    PARTIAL = "partial"    # Some steps succeeded, others failed
    CANCELLED = "cancelled"

class ProcessingJobType(str, Enum):
    FULL = "full"              # All processing steps
    TEXT_ONLY = "text_only"    # PDF extraction only
    VOCABULARY_ONLY = "vocabulary_only"
    AUDIO_ONLY = "audio_only"

class JobPriority(str, Enum):
    HIGH = "high"       # Admin re-processing, urgent
    NORMAL = "normal"   # Standard auto-processing
    LOW = "low"         # Bulk/batch processing

@dataclass
class ProcessingJob:
    job_id: str
    book_id: str
    publisher_id: str
    job_type: ProcessingJobType = ProcessingJobType.FULL
    status: ProcessingStatus = ProcessingStatus.QUEUED
    priority: JobPriority = JobPriority.NORMAL
    progress: int = 0               # 0-100 percentage
    current_step: str = ""          # Current processing step
    error_message: str | None = None
    retry_count: int = 0
    created_at: datetime = field(default_factory=datetime.utcnow)
    started_at: datetime | None = None
    completed_at: datetime | None = None
    metadata: dict = field(default_factory=dict)  # Additional job context

@dataclass
class QueueStats:
    total_jobs: int
    queued_jobs: int
    processing_jobs: int
    completed_jobs: int
    failed_jobs: int
    active_workers: int
```

### Error Hierarchy
```python
class QueueError(Exception):
    """Base exception for queue errors."""
    def __init__(self, message: str, details: dict | None = None):
        self.message = message
        self.details = details or {}
        super().__init__(message)

class JobNotFoundError(QueueError):
    """Raised when a job is not found in the queue."""
    def __init__(self, job_id: str):
        super().__init__(f"Job not found: {job_id}", {"job_id": job_id})

class JobAlreadyExistsError(QueueError):
    """Raised when trying to create a duplicate job."""
    def __init__(self, book_id: str):
        super().__init__(f"Job already exists for book: {book_id}", {"book_id": book_id})

class QueueConnectionError(QueueError):
    """Raised when Redis connection fails."""
    pass
```

[Source: Pattern from apps/api/app/services/llm/base.py:23-69, tts/base.py]

### Progress Reporting Design
```python
# Processing stages with weight percentages
PROCESSING_STAGES = {
    "text_extraction": 20,    # PDF → text files
    "segmentation": 15,       # Text → modules
    "topic_analysis": 20,     # AI topic extraction
    "vocabulary": 20,         # AI vocabulary extraction
    "audio_generation": 25,   # TTS for vocabulary
}

# Total: 100%

# Progress callback interface
async def report_progress(
    job_id: str,
    stage: str,
    stage_progress: int,  # 0-100 within stage
) -> None:
    """Update job progress based on stage and stage progress."""
    stage_weight = PROCESSING_STAGES[stage]
    stages_before = sum(
        weight for s, weight in PROCESSING_STAGES.items()
        if list(PROCESSING_STAGES.keys()).index(s) < list(PROCESSING_STAGES.keys()).index(stage)
    )
    overall_progress = stages_before + (stage_progress * stage_weight // 100)
    await job_repository.update_job_progress(job_id, overall_progress, current_step=stage)
```

### arq Worker Configuration
```python
# worker.py
from arq import cron
from arq.connections import RedisSettings

class WorkerSettings:
    functions = [process_book_task]
    redis_settings = RedisSettings.from_dsn(settings.redis_url)
    max_jobs = settings.queue_max_concurrency
    job_timeout = settings.queue_job_timeout_seconds
    retry_jobs = True
    max_tries = settings.queue_max_retries
    health_check_interval = 30

    # Retry with exponential backoff
    @staticmethod
    def retry_delay(attempt: int) -> float:
        return min(settings.queue_retry_delay_seconds * (2 ** attempt), 3600)
```

### Technical Notes from Epic
- Processing is async - don't block book upload API
- Webhook/callback when processing completes (future story integration)
- Admin can view queue status via API
- Jobs should be idempotent (safe to retry)
- Cleanup old job records automatically (TTL)

[Source: docs/prd/epic-10-ai-book-processing-pipeline.md#story-103]

## Testing

### Test Location
`apps/api/tests/test_queue_service.py`

### Testing Standards
- Use pytest with async support (`pytest-asyncio`)
- Mock Redis connections (use `fakeredis` or manual mocking)
- Test job lifecycle state transitions
- Test concurrent access patterns
- Follow patterns from `apps/api/tests/test_llm_service.py`, `test_tts_service.py`

[Source: docs/architecture/16-testing-strategy.md]

### Test Cases to Cover
1. Job creation with all fields
2. Job retrieval by ID
3. Status transitions: QUEUED → PROCESSING → COMPLETED
4. Status transitions: QUEUED → PROCESSING → FAILED (with retry)
5. Progress updates (0 → 50 → 100)
6. Priority queue ordering (HIGH before NORMAL before LOW)
7. Concurrent job limits
8. Job cancellation (queued and in-progress)
9. Duplicate job prevention for same book
10. Queue statistics calculation
11. Job cleanup after TTL
12. Error handling for Redis connection failures

### Mock Strategy
```python
# Example mock for Redis operations
@pytest.fixture
def mock_redis():
    """Mock Redis client for testing."""
    storage = {}

    class MockRedis:
        async def hset(self, key: str, mapping: dict) -> int:
            storage[key] = mapping
            return len(mapping)

        async def hget(self, key: str, field: str) -> str | None:
            return storage.get(key, {}).get(field)

        async def hgetall(self, key: str) -> dict:
            return storage.get(key, {})

        async def delete(self, key: str) -> int:
            return 1 if storage.pop(key, None) else 0

        async def expire(self, key: str, seconds: int) -> bool:
            return True

    return MockRedis()

@pytest.fixture
def job_repository(mock_redis):
    return JobRepository(redis_client=mock_redis)
```

## Dependencies

### New Python Packages
```toml
# Add to apps/api/pyproject.toml
dependencies = [
  # ... existing dependencies
  "arq>=0.26,<0.27",
  "redis>=5.0,<6.0"
]
```

### Infrastructure Requirements
- Redis server running (local: `redis://localhost:6379`)
- For production: Redis cluster or managed Redis (AWS ElastiCache, etc.)

## Dev Agent Record

### Agent Model Used
Claude Opus 4.5 (claude-opus-4-5-20251101)

### File List
| File | Action | Description |
|------|--------|-------------|
| `apps/api/app/services/queue/__init__.py` | Created | Module exports for queue service |
| `apps/api/app/services/queue/models.py` | Created | ProcessingJob, enums, exceptions |
| `apps/api/app/services/queue/redis.py` | Created | Redis connection management |
| `apps/api/app/services/queue/repository.py` | Created | JobRepository for Redis storage |
| `apps/api/app/services/queue/service.py` | Created | QueueService, ProgressReporter |
| `apps/api/app/services/queue/tasks.py` | Created | Worker task definitions |
| `apps/api/app/services/queue/worker.py` | Created | Worker runner configuration |
| `apps/api/app/core/config.py` | Modified | Added queue configuration settings |
| `apps/api/pyproject.toml` | Modified | Added arq and redis dependencies |
| `apps/api/tests/test_queue_service.py` | Created | 46 unit tests for queue system |
| `infrastructure/docker-compose.yml` | Modified | Added Redis service for queue system |

### Completion Notes
- All 9 tasks completed successfully
- 46 unit tests pass covering all acceptance criteria
- arq (Async Redis Queue) used for native async support
- Progress reporting implemented with 5 weighted stages
- Worker supports graceful shutdown and exponential backoff retry
- Pre-existing test failures (database connection) unrelated to this story

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2024-12-30 | 0.1 | Initial story draft | Bob (SM Agent) |
| 2024-12-30 | 1.0 | Implementation complete | James (Dev Agent) |

## QA Results

### Review Date: 2024-12-30

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment

**Excellent implementation.** The queue system is well-architected with clean separation of concerns following established project patterns (llm/tts service structure). Code demonstrates:

- Proper async/await patterns throughout all modules
- Clean dataclass models with timezone-aware datetime handling
- Comprehensive error hierarchy with contextual details
- Well-designed Redis repository with proper indexing and TTL management
- Weighted progress reporting across 5 processing stages
- Priority queue support with configurable concurrency

### Refactoring Performed

None required - implementation is clean and follows best practices.

### Compliance Check

- Coding Standards: ✓ Follows project async patterns, docstrings, type hints
- Project Structure: ✓ Matches services/{module}/ pattern from llm/tts
- Testing Strategy: ✓ 46 unit tests with mocked Redis, comprehensive coverage
- All ACs Met: ✓ All 6 acceptance criteria fully implemented

### Improvements Checklist

- [x] All code follows async/await patterns
- [x] Timezone-aware datetime (fixed deprecation warnings)
- [x] Comprehensive error handling with custom exceptions
- [x] Proper connection pooling and cleanup
- [x] Progress reporting with weighted stages
- [x] Priority queue implementation
- [x] Graceful worker shutdown
- [x] Exponential backoff retry logic
- [ ] Integration tests with real Redis (future story)
- [ ] Implement actual processing in _run_processing_stage() (future stories)

### Security Review

No security concerns identified:
- No hardcoded credentials
- Redis URL configurable via environment variables
- Proper error handling without leaking internal details
- Connection errors wrapped in custom exceptions

### Performance Considerations

Performance is well-addressed:
- Async throughout for non-blocking operations
- Connection pooling for Redis
- Configurable concurrency limits (DCS_QUEUE_MAX_CONCURRENCY)
- Job TTL for automatic cleanup of old records
- Efficient Redis data structures (hashes, sets)

### Files Modified During Review

None - no refactoring was necessary.

### Gate Status

Gate: **PASS** → docs/qa/gates/10.3-ai-processing-queue-system.yml

### Recommended Status

✓ **Ready for Done** - All acceptance criteria met, comprehensive test coverage, clean implementation following project patterns.
