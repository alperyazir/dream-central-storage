# Story 10.6: AI Topic Extraction

## Status
Complete

## Story
**As a** system developer,
**I want** an AI-powered topic extraction service that analyzes each module's text content,
**so that** the AI processing pipeline can extract educational topics, grammar points, difficulty levels, and language detection for consumption by Dream LMS.

## Acceptance Criteria
1. Analyze module text with LLM
2. Extract 3-5 key topics per module
3. Identify grammar points (for language books)
4. Detect difficulty level (A1-C2 for language)
5. Language detection
6. Store topics in module JSON

## Tasks / Subtasks

- [x] Task 1: Create topic analysis service module structure (AC: 1-6)
  - [x] Create `apps/api/app/services/topic_analysis/__init__.py` module
  - [x] Create `apps/api/app/services/topic_analysis/models.py` with dataclasses
  - [x] Define `TopicResult` dataclass: topics (list[str]), grammar_points (list[str]), difficulty (str), language (str), target_skills (list[str])
  - [x] Define `ModuleAnalysisResult` dataclass: module_id, topic_result, analyzed_at, llm_provider, tokens_used
  - [x] Define `BookAnalysisResult` dataclass: book_id, publisher_id, book_name, module_results (list), primary_language, difficulty_range
  - [x] Define exceptions: `TopicAnalysisError`, `LLMAnalysisError`, `NoModulesFoundError`

- [x] Task 2: Create LLM prompt templates (AC: 1-4)
  - [x] Create `apps/api/app/services/topic_analysis/prompts.py`
  - [x] Define `TOPIC_EXTRACTION_PROMPT` template for topic analysis
  - [x] Define `GRAMMAR_EXTRACTION_PROMPT` template for grammar points
  - [x] Define `DIFFICULTY_DETECTION_PROMPT` template for level detection
  - [x] Include JSON output format instructions in prompts
  - [x] Support bilingual book analysis (English + Turkish)

- [x] Task 3: Implement topic analysis service (AC: 1-6)
  - [x] Create `apps/api/app/services/topic_analysis/service.py`
  - [x] Implement `TopicAnalysisService` class
  - [x] Implement `analyze_module()` - analyze single module text with LLM
  - [x] Implement `analyze_book()` - analyze all modules for a book
  - [x] Implement `get_topic_analysis_service()` singleton factory
  - [x] Use `LLMService` from Story 10.1 for AI analysis
  - [x] Parse LLM JSON responses to extract structured data
  - [x] Handle LLM failures gracefully with retries and fallback

- [x] Task 4: Implement language detection (AC: 5)
  - [x] Add language detection logic to `analyze_module()`
  - [x] Support English, Turkish, and bilingual detection
  - [x] Use LLM for language identification from text content
  - [x] Aggregate primary language across modules for book-level detection

- [x] Task 5: Implement difficulty level detection (AC: 4)
  - [x] Define CEFR levels: A1, A2, B1, B2, C1, C2
  - [x] Add difficulty detection to analysis prompt
  - [x] Parse difficulty from LLM response
  - [x] Calculate difficulty range for entire book

- [x] Task 6: Implement module JSON update storage (AC: 6)
  - [x] Create `apps/api/app/services/topic_analysis/storage.py`
  - [x] Implement `TopicStorage` class
  - [x] Implement `update_module_with_topics()` - update existing module JSON with topics
  - [x] Load module from `ai-data/modules/module_N.json`
  - [x] Update fields: topics, language, difficulty
  - [x] Save updated module back to MinIO
  - [x] Implement `get_topic_storage()` singleton factory

- [x] Task 7: Add topic analysis configuration settings (AC: 1, 2)
  - [x] Add settings to `apps/api/app/core/config.py`
  - [x] Add `DCS_TOPIC_ANALYSIS_MAX_TOPICS` (default: 5)
  - [x] Add `DCS_TOPIC_ANALYSIS_MAX_GRAMMAR_POINTS` (default: 10)
  - [x] Add `DCS_TOPIC_ANALYSIS_TEMPERATURE` (default: 0.3)
  - [x] Add `DCS_TOPIC_ANALYSIS_MAX_TEXT_LENGTH` (default: 8000)

- [x] Task 8: Write unit tests (AC: 1-6)
  - [x] Create `apps/api/tests/test_topic_analysis.py`
  - [x] Test topic extraction with mocked LLM
  - [x] Test grammar point extraction
  - [x] Test difficulty level detection
  - [x] Test language detection
  - [x] Test module JSON update
  - [x] Test error handling for LLM failures
  - [x] Test prompts generate valid JSON responses
  - [x] Mock LLMService for isolated testing

- [x] Task 9: Integration with queue system (AC: 1-6)
  - [x] Update `apps/api/app/services/queue/tasks.py`
  - [x] Implement `_run_topic_analysis()` function for topic_analysis stage
  - [x] Receive segmentation result from previous stage
  - [x] Iterate through all modules and analyze each
  - [x] Update module JSONs with analysis results
  - [x] Pass analysis summary to next stage (vocabulary)
  - [x] Wire progress reporting from topic analysis to queue progress

## Dev Notes

### Previous Story Insights (Story 10.5)
- **Pattern Used:** Module structure follows `services/{module}/` pattern with `models.py`, `service.py`, `__init__.py`
- **Singleton Pattern:** `get_segmentation_service()` factory function for global instance
- **Exception Hierarchy:** Specific exceptions inherit from base error class with book_id context
- **Async Patterns:** Using async/await throughout services
- **Storage:** `ModuleStorage` class in `segmentation/storage.py` handles MinIO operations for modules
- **Queue Integration:** Add stage implementation in `_run_processing_stage()` function in tasks.py
- **Progress Callback:** Sync callback that tracks progress, final async report

[Source: Story 10.5 Dev Agent Record]

### Data Models (from Epic 10)
Module JSON structure at `ai-data/modules/module_N.json`:
```json
{
  "module_id": 1,
  "title": "Unit 1: Greetings",
  "pages": [1, 2, 3, 4, 5],
  "text": "Full text content of the module...",
  "topics": ["greetings", "introductions", "basic phrases"],  // THIS STORY POPULATES
  "vocabulary_ids": [],  // Populated by Story 10.7
  "language": "en",  // THIS STORY POPULATES
  "difficulty": "A1",  // THIS STORY POPULATES
  "word_count": 450,
  "extracted_at": "2024-01-15T10:30:00Z"
}
```

[Source: docs/prd/epic-10-ai-book-processing-pipeline.md#data-schemas]

### Storage Structure
```
/publishers/{publisher_id}/books/{book_id}/{book_name}/
└── ai-data/
    ├── text/                    # From Story 10.4
    │   ├── page_001.txt
    │   └── extraction_metadata.json
    └── modules/                 # From Story 10.5
        ├── module_1.json        # UPDATED by this story
        ├── module_2.json        # UPDATED by this story
        └── segmentation_metadata.json
```

[Source: docs/prd/epic-10-ai-book-processing-pipeline.md#storage-structure]

### LLM Service Usage (from Story 10.1)
```python
from app.services.llm import get_llm_service, LLMRequest

llm_service = get_llm_service()

# Simple completion
response = await llm_service.simple_completion(
    prompt="Analyze this text...",
    system_prompt="You are an educational content analyzer...",
    temperature=0.3,
    max_tokens=2048,
)

# Or using chat method
response = await llm_service.chat(
    messages=[
        LLMMessage(role="system", content="..."),
        LLMMessage(role="user", content="..."),
    ],
    temperature=0.3,
)
```

[Source: apps/api/app/services/llm/service.py:293-317]

### Prompt Template (from Epic 10)
```python
TOPIC_EXTRACTION_PROMPT = """Analyze this educational content and extract:
1. Main topics (3-5)
2. Grammar points (if language learning)
3. Difficulty level (A1/A2/B1/B2/C1/C2)
4. Target skills (reading, writing, speaking, listening)

Content:
{module_text}

Return as JSON:
{
  "topics": ["topic1", "topic2", ...],
  "grammar_points": ["point1", "point2", ...],
  "difficulty": "A1",
  "language": "en",
  "target_skills": ["reading", "listening"]
}
"""
```

[Source: docs/prd/epic-10-ai-book-processing-pipeline.md#story-106]

### Queue Stage Integration
The topic_analysis stage has 20% weight in the processing pipeline.
```python
PROCESSING_STAGES = {
    "text_extraction": 20,  # Story 10.4 ✓
    "segmentation": 15,     # Story 10.5 ✓
    "topic_analysis": 20,   # THIS STORY
    "vocabulary": 20,
    "audio_generation": 25,
}
```

[Source: apps/api/app/services/queue/models.py:74-80]

### Existing Module Model Fields
The `Module` dataclass already has fields for topics, language, and difficulty:
```python
@dataclass
class Module:
    module_id: int
    title: str
    pages: list[int]
    start_page: int
    end_page: int
    text: str = ""
    word_count: int = field(init=False)
    topics: list[str] = field(default_factory=list)  # Populate this
    vocabulary_ids: list[str] = field(default_factory=list)
    language: str = ""  # Populate this
    difficulty: str = ""  # Populate this
```

[Source: apps/api/app/services/segmentation/models.py:85-101]

### Existing Dependencies
- `LLMService` from Story 10.1 (for AI analysis)
- `ModuleStorage` from Story 10.5 (for loading/saving modules)
- `SegmentationResult` from Story 10.5 (passed from previous stage)
- `QueueService` from Story 10.3 (for job integration)
- MinIO service for storage operations

[Source: apps/api/app/services/llm/, segmentation/, queue/]

### Project File Structure
New files to create:
```
apps/api/app/services/topic_analysis/
├── __init__.py              # Module exports
├── models.py                # TopicResult, ModuleAnalysisResult, exceptions
├── prompts.py               # LLM prompt templates
├── service.py               # TopicAnalysisService
└── storage.py               # TopicStorage for module updates
```

[Source: Pattern from apps/api/app/services/segmentation/]

### Configuration Settings Pattern
Settings use `DCS_` prefix and are defined in `config.py`:
```python
# Topic Analysis Configuration
topic_analysis_max_topics: int = 5
topic_analysis_max_grammar_points: int = 10
topic_analysis_temperature: float = 0.3
topic_analysis_max_text_length: int = 8000
```

[Source: Pattern from apps/api/app/core/config.py:94-98]

## Testing

### Test Location
`apps/api/tests/test_topic_analysis.py`

### Testing Standards
- Use pytest with async support (`pytest-asyncio`)
- Mock MinIO storage operations
- Mock LLMService calls for AI analysis tests
- Use sample module text fixtures
- Follow patterns from `apps/api/tests/test_segmentation.py`

[Source: docs/architecture/16-testing-strategy.md]

### Test Cases to Cover
1. Topic extraction from educational text
2. Grammar point identification for language books
3. CEFR difficulty level detection (A1-C2)
4. Language detection (English, Turkish, bilingual)
5. Module JSON update with topics
6. LLM failure handling and retries
7. JSON response parsing from LLM
8. Empty/minimal text handling
9. Multi-module book analysis
10. Progress reporting during analysis

## Dependencies

### Python Packages
No new packages required - uses existing:
- `httpx` (for LLM service)
- `minio` (for storage)

### Existing Services Used
- `LLMService` from Story 10.1
- `ModuleStorage` from Story 10.5
- `QueueService` from Story 10.3

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2024-12-30 | 0.1 | Initial story draft | Bob (SM Agent) |
| 2024-12-30 | 1.0 | Implementation complete | James (Dev Agent) |

## Dev Agent Record

### Agent Model Used
Claude Opus 4.5 (claude-opus-4-5-20251101)

### Debug Log References
N/A - No debug logs generated

### Completion Notes List
- All 9 tasks completed successfully
- 45 unit tests written and passing
- Topic analysis service integrated with queue processing pipeline
- All acceptance criteria met:
  - AC1: Module text analyzed with LLM via TopicAnalysisService.analyze_module()
  - AC2: Topics extracted (3-5 per module, configurable)
  - AC3: Grammar points identified via LLM prompt
  - AC4: CEFR difficulty levels (A1-C2) detected
  - AC5: Language detection (en, tr, bilingual) implemented
  - AC6: Topics stored in module JSON via TopicStorage.update_module_with_topics()

### File List

**Created:**
- `apps/api/app/services/topic_analysis/__init__.py` - Module exports
- `apps/api/app/services/topic_analysis/models.py` - Data models and exceptions
- `apps/api/app/services/topic_analysis/prompts.py` - LLM prompt templates
- `apps/api/app/services/topic_analysis/service.py` - TopicAnalysisService
- `apps/api/app/services/topic_analysis/storage.py` - TopicStorage for module updates
- `apps/api/tests/test_topic_analysis.py` - Unit tests (45 tests)

**Modified:**
- `apps/api/app/core/config.py` - Added topic analysis settings
- `apps/api/app/services/queue/tasks.py` - Added _run_topic_analysis() stage

## QA Results

### Review Date
2024-12-30

### Reviewer
Quinn (QA Agent) - Claude Opus 4.5

### Overall Assessment
**PASSED** ✅

### Acceptance Criteria Verification

| AC | Description | Status | Evidence |
|----|-------------|--------|----------|
| AC1 | Analyze module text with LLM | ✅ Pass | `TopicAnalysisService.analyze_module()` in `service.py:151-303` uses LLM for content analysis |
| AC2 | Extract 3-5 key topics per module | ✅ Pass | Topics extracted via LLM prompt, limited by `topic_analysis_max_topics` setting (default: 5) |
| AC3 | Identify grammar points | ✅ Pass | Grammar points extracted via `TOPIC_EXTRACTION_PROMPT`, stored in `TopicResult.grammar_points` |
| AC4 | Detect difficulty level (A1-C2) | ✅ Pass | CEFR levels validated in `_extract_topic_result()` at `service.py:136-137` |
| AC5 | Language detection | ✅ Pass | Language detection (en, tr, bilingual) in `service.py:139-141` |
| AC6 | Store topics in module JSON | ✅ Pass | `TopicStorage.update_module_with_topics()` in `storage.py:91-142` updates MinIO |

### Code Quality Assessment

| Category | Rating | Notes |
|----------|--------|-------|
| Architecture | Excellent | Follows established service pattern from segmentation module |
| Error Handling | Excellent | Graceful fallback with simple prompt on LLM failure |
| Type Safety | Good | Proper type hints throughout, dataclasses for models |
| Logging | Excellent | Comprehensive logging at all critical points |
| Configuration | Excellent | 4 new settings with sensible defaults |

### Test Coverage

| Test Category | Tests | Status |
|---------------|-------|--------|
| Model tests | 15 | ✅ Pass |
| Prompt tests | 6 | ✅ Pass |
| Service tests | 14 | ✅ Pass |
| Storage tests | 10 | ✅ Pass |
| **Total** | **45** | **✅ All Pass** |

### NFR Assessment

| NFR | Status | Evidence |
|-----|--------|----------|
| Security | ✅ Pass | No secrets in code, proper input sanitization |
| Performance | ✅ Pass | Configurable text length limit (8000 chars default) |
| Reliability | ✅ Pass | Fallback prompt on LLM failure, graceful error handling |
| Maintainability | ✅ Pass | Clean separation of concerns, singleton pattern |

### Issues Found
None - implementation meets all requirements.

### Recommendations
1. Consider adding integration tests with real LLM calls in a dedicated test suite (not blocking)
2. Monitor token usage in production for cost optimization (future enhancement)

### Sign-off
Story 10.6 is **approved for completion**. All acceptance criteria verified, tests passing, code quality excellent.
