# Story 10.7: AI Vocabulary Extraction

## Status
Done

## Story
**As a** system developer,
**I want** an AI-powered vocabulary extraction service that identifies and extracts vocabulary words from each module,
**so that** the AI processing pipeline can build a master vocabulary list with definitions, translations, and metadata for consumption by Dream LMS.

## Acceptance Criteria
1. Identify vocabulary words in each module
2. Generate definitions for each word
3. Provide translation (if bilingual book)
4. Detect part of speech
5. Extract example sentences from book
6. Assign difficulty level per word
7. Deduplicate across modules
8. Create master vocabulary.json

## Tasks / Subtasks

- [x] Task 1: Create vocabulary extraction service module structure (AC: 1-8)
  - [x] Create `apps/api/app/services/vocabulary_extraction/__init__.py` module
  - [x] Create `apps/api/app/services/vocabulary_extraction/models.py` with dataclasses
  - [x] Define `VocabularyWord` dataclass: id, word, translation, definition, part_of_speech, level, example, module_id, page, audio (dict)
  - [x] Define `ModuleVocabularyResult` dataclass: module_id, words (list), extracted_at, llm_provider, tokens_used, success, error_message
  - [x] Define `BookVocabularyResult` dataclass: book_id, publisher_id, book_name, language, translation_language, total_words, words (list), module_results (list), extracted_at
  - [x] Define exceptions: `VocabularyExtractionError`, `LLMExtractionError`, `NoModulesFoundError`, `DuplicateVocabularyError`

- [x] Task 2: Create LLM prompt templates (AC: 1-6)
  - [x] Create `apps/api/app/services/vocabulary_extraction/prompts.py`
  - [x] Define `VOCABULARY_EXTRACTION_PROMPT` template for word extraction
  - [x] Define `SYSTEM_PROMPT` for vocabulary extraction context
  - [x] Define `SIMPLE_VOCABULARY_PROMPT` as fallback for simpler extraction
  - [x] Include JSON output format instructions in prompts
  - [x] Support difficulty level filtering in prompts
  - [x] Support bilingual extraction (English + Turkish)

- [x] Task 3: Implement vocabulary extraction service (AC: 1-6)
  - [x] Create `apps/api/app/services/vocabulary_extraction/service.py`
  - [x] Implement `VocabularyExtractionService` class
  - [x] Implement `extract_module_vocabulary()` - extract vocabulary from single module text with LLM
  - [x] Implement `extract_book_vocabulary()` - extract vocabulary from all modules
  - [x] Implement `get_vocabulary_extraction_service()` singleton factory
  - [x] Use `LLMService` from Story 10.1 for AI extraction
  - [x] Parse LLM JSON responses to extract structured vocabulary data
  - [x] Handle LLM failures gracefully with retries and fallback prompt

- [x] Task 4: Implement deduplication logic (AC: 7)
  - [x] Add `_deduplicate_vocabulary()` method to service
  - [x] Track word occurrences across modules
  - [x] Keep first occurrence, merge module references
  - [x] Create unique word IDs (slugified word)
  - [x] Handle case-insensitive matching

- [x] Task 5: Implement vocabulary storage (AC: 8)
  - [x] Create `apps/api/app/services/vocabulary_extraction/storage.py`
  - [x] Implement `VocabularyStorage` class
  - [x] Implement `save_vocabulary()` - save master vocabulary.json to MinIO
  - [x] Implement `load_vocabulary()` - load existing vocabulary if any
  - [x] Implement `update_module_vocabulary_ids()` - update module JSONs with vocabulary_ids
  - [x] Save to `ai-data/vocabulary.json`
  - [x] Implement `get_vocabulary_storage()` singleton factory

- [x] Task 6: Add vocabulary extraction configuration settings (AC: 1, 6)
  - [x] Add settings to `apps/api/app/core/config.py`
  - [x] Add `DCS_VOCABULARY_MAX_WORDS_PER_MODULE` (default: 50)
  - [x] Add `DCS_VOCABULARY_MIN_WORD_LENGTH` (default: 3)
  - [x] Add `DCS_VOCABULARY_TEMPERATURE` (default: 0.3)
  - [x] Add `DCS_VOCABULARY_MAX_TEXT_LENGTH` (default: 8000)

- [x] Task 7: Write unit tests (AC: 1-8)
  - [x] Create `apps/api/tests/test_vocabulary_extraction.py`
  - [x] Test vocabulary extraction with mocked LLM
  - [x] Test definition generation
  - [x] Test translation extraction
  - [x] Test part of speech detection
  - [x] Test example sentence extraction
  - [x] Test difficulty level assignment
  - [x] Test deduplication logic
  - [x] Test vocabulary.json creation and storage
  - [x] Test error handling for LLM failures
  - [x] Mock LLMService for isolated testing

- [x] Task 8: Integration with queue system (AC: 1-8)
  - [x] Update `apps/api/app/services/queue/tasks.py`
  - [x] Implement `_run_vocabulary_extraction()` function for vocabulary stage
  - [x] Receive topic_analysis result from previous stage
  - [x] Iterate through all modules and extract vocabulary
  - [x] Deduplicate vocabulary across modules
  - [x] Save vocabulary.json and update module JSONs
  - [x] Pass vocabulary summary to next stage (audio_generation)
  - [x] Wire progress reporting from vocabulary extraction to queue progress

## Dev Notes

### Previous Story Insights (Story 10.6)
- **Pattern Used:** Module structure follows `services/{module}/` pattern with `models.py`, `prompts.py`, `service.py`, `storage.py`, `__init__.py`
- **Singleton Pattern:** `get_*_service()` factory function for global instance
- **Exception Hierarchy:** Specific exceptions inherit from base error class with book_id context
- **Async Patterns:** Using async/await throughout services
- **Storage:** TopicStorage class pattern - loads from MinIO, updates, saves back
- **Queue Integration:** Add stage implementation in `_run_processing_stage()` function in tasks.py
- **Progress Callback:** Sync callback that tracks progress, final async report
- **LLM Usage:** `llm_service.simple_completion()` with system_prompt, temperature, max_tokens
- **JSON Parsing:** Extract JSON from LLM response using regex, handle parsing errors gracefully
- **Fallback Strategy:** If main prompt fails, try simpler prompt before returning error

[Source: Story 10.6 Dev Agent Record]

### Data Models (from Epic 10)

#### Vocabulary JSON structure at `ai-data/vocabulary.json`:
```json
{
  "language": "en",
  "translation_language": "tr",
  "total_words": 150,
  "words": [
    {
      "id": "beautiful",
      "word": "beautiful",
      "translation": "güzel",
      "definition": "pleasing to the senses or mind aesthetically",
      "part_of_speech": "adjective",
      "level": "A2",
      "example": "It's a beautiful day today.",
      "module_id": 3,
      "page": 24,
      "audio": {
        "word": "audio/vocabulary/en/beautiful.mp3",
        "translation": "audio/vocabulary/tr/guzel.mp3"
      }
    }
  ],
  "extracted_at": "2024-01-15T10:30:00Z"
}
```

[Source: docs/prd/epic-10-ai-book-processing-pipeline.md#data-schemas]

#### Module JSON structure (update vocabulary_ids field):
```json
{
  "module_id": 1,
  "title": "Unit 1: Greetings",
  "pages": [1, 2, 3, 4, 5],
  "text": "Full text content...",
  "topics": ["greetings", "introductions"],
  "vocabulary_ids": ["hello", "goodbye", "nice_to_meet_you"],  // THIS STORY POPULATES
  "language": "en",
  "difficulty": "A1",
  "word_count": 450,
  "extracted_at": "2024-01-15T10:30:00Z"
}
```

[Source: docs/prd/epic-10-ai-book-processing-pipeline.md#data-schemas]

### Storage Structure
```
/publishers/{publisher_id}/books/{book_id}/{book_name}/
└── ai-data/
    ├── text/                    # From Story 10.4
    ├── modules/                 # From Story 10.5, updated by Story 10.6
    │   ├── module_1.json        # UPDATED by this story (vocabulary_ids)
    │   ├── module_2.json        # UPDATED by this story (vocabulary_ids)
    │   └── segmentation_metadata.json
    └── vocabulary.json          # CREATED by this story
```

[Source: docs/prd/epic-10-ai-book-processing-pipeline.md#storage-structure]

### LLM Service Usage (from Story 10.1)
```python
from app.services.llm import get_llm_service, LLMRequest

llm_service = get_llm_service()

# Simple completion
response = await llm_service.simple_completion(
    prompt="Extract vocabulary from this text...",
    system_prompt="You are an educational vocabulary extractor...",
    temperature=0.3,
    max_tokens=2048,
)
```

[Source: apps/api/app/services/llm/service.py:293-317]

### Prompt Template (from Epic 10)
```python
VOCABULARY_EXTRACTION_PROMPT = """Extract vocabulary from this educational text:
- Identify key vocabulary words for language learners
- For each word provide: definition, part of speech, example from text
- Focus on words at {difficulty} level
- Provide Turkish translation if applicable

Text:
{module_text}

Return as JSON array:
[
  {{
    "word": "beautiful",
    "translation": "güzel",
    "definition": "pleasing to the senses aesthetically",
    "part_of_speech": "adjective",
    "level": "A2",
    "example": "It's a beautiful day today."
  }}
]
"""
```

[Source: docs/prd/epic-10-ai-book-processing-pipeline.md#story-107]

### Queue Stage Integration
The vocabulary stage has 20% weight in the processing pipeline.
```python
PROCESSING_STAGES = {
    "text_extraction": 20,  # Story 10.4 ✓
    "segmentation": 15,     # Story 10.5 ✓
    "topic_analysis": 20,   # Story 10.6 ✓
    "vocabulary": 20,       # THIS STORY
    "audio_generation": 25,
}
```

[Source: apps/api/app/services/queue/models.py:74-80]

### Existing Dependencies
- `LLMService` from Story 10.1 (for AI extraction)
- `TopicStorage` from Story 10.6 (for loading modules with topics)
- `QueueService` from Story 10.3 (for job integration)
- MinIO service for storage operations

[Source: apps/api/app/services/llm/, topic_analysis/, queue/]

### Project File Structure
New files to create:
```
apps/api/app/services/vocabulary_extraction/
├── __init__.py              # Module exports
├── models.py                # VocabularyWord, ModuleVocabularyResult, exceptions
├── prompts.py               # LLM prompt templates
├── service.py               # VocabularyExtractionService
└── storage.py               # VocabularyStorage for vocabulary.json
```

[Source: Pattern from apps/api/app/services/topic_analysis/]

### Configuration Settings Pattern
Settings use `DCS_` prefix and are defined in `config.py`:
```python
# Vocabulary Extraction Configuration
vocabulary_max_words_per_module: int = 50
vocabulary_min_word_length: int = 3
vocabulary_temperature: float = 0.3
vocabulary_max_text_length: int = 8000
```

[Source: Pattern from apps/api/app/core/config.py]

## Testing

### Test Location
`apps/api/tests/test_vocabulary_extraction.py`

### Testing Standards
- Use pytest with async support (`pytest-asyncio`)
- Mock MinIO storage operations
- Mock LLMService calls for AI extraction tests
- Use sample module text fixtures
- Follow patterns from `apps/api/tests/test_topic_analysis.py`

[Source: docs/architecture/16-testing-strategy.md]

### Test Cases to Cover
1. Vocabulary word identification from educational text
2. Definition generation for each word
3. Translation extraction for bilingual content
4. Part of speech detection
5. Example sentence extraction from source text
6. Difficulty level assignment (A1-C2)
7. Deduplication across multiple modules
8. Vocabulary.json creation with correct schema
9. Module JSON update with vocabulary_ids
10. LLM failure handling and fallback
11. JSON response parsing from LLM
12. Empty/minimal text handling
13. Multi-module book vocabulary extraction
14. Progress reporting during extraction

## Dependencies

### Python Packages
No new packages required - uses existing:
- `httpx` (for LLM service)
- `minio` (for storage)

### Existing Services Used
- `LLMService` from Story 10.1
- `TopicStorage` from Story 10.6 (for loading modules)
- `QueueService` from Story 10.3

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|-----------|
| 2024-12-30 | 0.1 | Initial story draft | Bob (SM Agent) |

## Dev Agent Record

### Agent Model Used
Claude Opus 4.5 (claude-opus-4-5-20251101)

### File List
**Created:**
- `apps/api/app/services/vocabulary_extraction/__init__.py` - Module exports
- `apps/api/app/services/vocabulary_extraction/models.py` - VocabularyWord, ModuleVocabularyResult, BookVocabularyResult, PartOfSpeech, exceptions
- `apps/api/app/services/vocabulary_extraction/prompts.py` - LLM prompt templates (VOCABULARY_EXTRACTION_PROMPT, SIMPLE_VOCABULARY_PROMPT, BILINGUAL_VOCABULARY_PROMPT)
- `apps/api/app/services/vocabulary_extraction/service.py` - VocabularyExtractionService with LLM integration
- `apps/api/app/services/vocabulary_extraction/storage.py` - VocabularyStorage for MinIO operations
- `apps/api/tests/test_vocabulary_extraction.py` - 47 unit tests

**Modified:**
- `apps/api/app/core/config.py` - Added vocabulary extraction config settings
- `apps/api/app/services/queue/tasks.py` - Added vocabulary stage implementation

### Completion Notes
- Vocabulary extraction service follows pattern from topic_analysis (Story 10.6)
- VocabularyWord uses slugified word as ID for deduplication
- Deduplication is case-insensitive, keeps first occurrence
- Module JSONs updated with vocabulary_ids array
- Master vocabulary.json saved to ai-data/ directory
- Queue integration receives language from topic_analysis stage
- All 47 tests passing with mocked LLM and MinIO
- Ruff linting passes

### Debug Log References
None - implementation completed without significant issues

### Change Log
| Date | Change | Files |
|------|--------|-------|
| 2024-12-30 | Created vocabulary extraction module structure | models.py, __init__.py |
| 2024-12-30 | Implemented LLM prompt templates | prompts.py |
| 2024-12-30 | Implemented extraction service with deduplication | service.py |
| 2024-12-30 | Implemented MinIO storage operations | storage.py |
| 2024-12-30 | Added config settings | config.py |
| 2024-12-30 | Created 47 unit tests | test_vocabulary_extraction.py |
| 2024-12-30 | Integrated with queue system | tasks.py |

## QA Results

### Review Date: 2024-12-30

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment

**Overall: Excellent** - The implementation follows established patterns from Story 10.6 (topic_analysis) with clean, well-documented code. All 8 acceptance criteria are fully implemented with comprehensive test coverage.

**Strengths:**
- Clean dataclass-based models with proper serialization (to_dict/from_dict)
- Well-structured exception hierarchy with contextual information (book_id, module_id)
- Robust LLM response parsing with fallback strategy
- Case-insensitive deduplication preserves first occurrence
- Good input validation (CEFR levels, part of speech, word length)
- Proper async/await patterns throughout
- Comprehensive docstrings on all public methods

**Architecture:**
- Module structure correctly follows `services/{module}/` pattern
- Singleton factories properly implemented
- Storage class follows TopicStorage patterns from Story 10.6
- Queue integration properly wired in `_run_processing_stage()`

### Refactoring Performed

None required - implementation is clean and follows established patterns.

### Compliance Check

- Coding Standards: ✓ Ruff linting passes, consistent style
- Project Structure: ✓ Follows `services/vocabulary_extraction/` pattern
- Testing Strategy: ✓ 47 unit tests with mocked dependencies
- All ACs Met: ✓ All 8 acceptance criteria fully implemented

### Improvements Checklist

All items complete - no changes required:

- [x] Data models with proper serialization (VocabularyWord, ModuleVocabularyResult, BookVocabularyResult)
- [x] Exception hierarchy with context (VocabularyExtractionError, LLMExtractionError, etc.)
- [x] LLM prompt templates with fallback strategy
- [x] Service with deduplication logic
- [x] MinIO storage operations for vocabulary.json
- [x] Module JSON updates with vocabulary_ids
- [x] Queue integration with progress reporting
- [x] Configuration settings (4 new settings in config.py)
- [x] Comprehensive unit tests (47 tests passing)

### Security Review

**Status: PASS**
- No user input directly exposed to LLM prompts without processing
- Error messages don't leak internal system details
- Text truncation prevents memory issues with large content
- No hardcoded credentials or secrets

### Performance Considerations

**Status: PASS**
- Text truncation configured via `vocabulary_max_text_length` (default 8000)
- Maximum words per module configurable via `vocabulary_max_words_per_module` (default 50)
- Fallback prompt uses reduced max_words (50%) and max_length (50%) for efficiency
- Deduplication uses dict for O(1) lookup

### Files Modified During Review

None - no changes made during review.

### Gate Status

Gate: **PASS** → docs/qa/gates/10.7-ai-vocabulary-extraction.yml

### Recommended Status

✓ **Ready for Done** - All acceptance criteria met, tests passing, code quality excellent.

(Story owner decides final status)
