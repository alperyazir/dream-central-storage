# Story 10.12: Auto-Processing on Upload

## Status
Done

## Story
**As a** Dream LMS publisher,
**I want** AI processing to automatically start when I upload a new book,
**so that** the book's content is immediately analyzed and ready for learning features without manual intervention.

## Acceptance Criteria

1. Hook into book upload completion event to trigger processing
2. Queue AI processing job automatically after successful upload
3. Configurable: auto-process on/off per publisher (via config setting)
4. Skip if book already processed (unless forced via override flag)
5. Notify admin on processing completion (via existing webhook system)
6. Handle upload of updated book (re-process when book content changes)

## Tasks / Subtasks

- [x] Task 1: Add auto-processing configuration to Settings (AC: 3)
  - [x] Add `ai_auto_process_on_upload` setting to `app/core/config.py`
  - [x] Default to `True` for automatic processing
  - [x] Add `ai_auto_process_skip_existing` setting to control skip behavior

- [x] Task 2: Create auto-processing service (AC: 1, 2, 4, 6)
  - [x] Create `apps/api/app/services/ai_processing/__init__.py`
  - [x] Create `apps/api/app/services/ai_processing/auto_trigger.py`
  - [x] Implement `AutoProcessingService` class with methods:
    - `should_auto_process(book_id, publisher_id)` - check if auto-processing is enabled
    - `is_already_processed(book_id)` - check if metadata.json exists
    - `trigger_processing(book_id, publisher_id, book_name, force=False)` - enqueue job
  - [x] Handle force re-processing when `override=True` on upload

- [x] Task 3: Integrate with book upload endpoints (AC: 1, 2, 6)
  - [x] Modify `POST /books/upload` endpoint in `apps/api/app/routers/books.py`
  - [x] After successful upload, call `AutoProcessingService.trigger_processing()`
  - [x] Modify `POST /books/{book_id}/upload` (replace content) to trigger re-processing
  - [x] Use `BackgroundTasks` for non-blocking processing trigger
  - [x] Handle upload with `override=True` to force re-processing

- [x] Task 4: Integrate with bulk upload endpoint (AC: 1, 2)
  - [x] Modify `POST /books/upload-bulk` endpoint
  - [x] Trigger auto-processing for each successfully uploaded book
  - [x] Log processing trigger for each book in bulk results

- [x] Task 5: Add webhook notification for processing completion (AC: 5)
  - [x] Add `PROCESSING_COMPLETED` event type to `WebhookEventType` enum
  - [x] Add `PROCESSING_FAILED` event type for failure notifications
  - [x] Create webhook payload schema for processing events
  - [x] Integrate webhook broadcast in processing job completion handler

- [x] Task 6: Write unit tests (AC: 1-6)
  - [x] Create `apps/api/tests/test_auto_processing.py`
  - [x] Test auto-processing triggers on single book upload
  - [x] Test auto-processing triggers on bulk upload
  - [x] Test skip behavior when book already processed
  - [x] Test force re-processing with override flag
  - [x] Test disabled auto-processing (config off)
  - [x] Mock queue service for isolated tests

## Dev Notes

### Previous Story Insights (Story 10.11)
- AIDataRetrievalService provides `get_metadata()` to check if a book has been processed
- Processing status is stored in `{publisher}/books/{book_name}/ai-data/metadata.json`
- Existing webhook pattern uses `BackgroundTasks` for non-blocking broadcast

### Existing Upload Flow (from `books.py`)
The current upload endpoints already use `BackgroundTasks` for webhooks:
```python
# Single upload - POST /books/upload
background_tasks.add_task(_trigger_webhook, book.id, WebhookEventType.BOOK_CREATED)

# Bulk upload - POST /books/upload-bulk
background_tasks.add_task(_trigger_webhook, book.id, WebhookEventType.BOOK_CREATED)

# Replace content - POST /books/{book_id}/upload
background_tasks.add_task(_trigger_webhook, book_id, WebhookEventType.BOOK_UPDATED)
```

The auto-processing trigger should follow the same pattern.

### Queue Service API (from Story 10.10)
```python
from app.services.queue import get_queue_service, ProcessingJobType, JobPriority

queue_service = await get_queue_service()
job = await queue_service.enqueue_job(
    book_id=str(book.id),
    publisher_id=book.publisher,
    job_type=ProcessingJobType.FULL,
    priority=JobPriority.NORMAL,
    metadata={"book_name": book.book_name, "auto_triggered": True},
)
```

### Checking if Book is Processed (from Story 10.11)
```python
from app.services.ai_data import get_ai_data_retrieval_service

retrieval_service = get_ai_data_retrieval_service()
metadata = retrieval_service.get_metadata(publisher, str(book_id), book_name)
if metadata is not None:
    # Book has been processed
```

### Configuration Pattern (from `config.py`)
Add new settings following existing pattern:
```python
# Auto-Processing Configuration
ai_auto_process_on_upload: bool = True
ai_auto_process_skip_existing: bool = True
```

### Webhook Event Types (from existing code)
Current webhook events in `app/models/webhook.py`:
- `BOOK_CREATED`
- `BOOK_UPDATED`
- `BOOK_DELETED`

Add new events:
- `PROCESSING_STARTED`
- `PROCESSING_COMPLETED`
- `PROCESSING_FAILED`

### File Locations
- Config: `apps/api/app/core/config.py`
- Books router: `apps/api/app/routers/books.py`
- New service: `apps/api/app/services/ai_processing/auto_trigger.py`
- Webhook models: `apps/api/app/models/webhook.py`
- Tests: `apps/api/tests/test_auto_processing.py`

## Testing

### Test File Location
`apps/api/tests/test_auto_processing.py`

### Testing Standards
- Use pytest with mocked dependencies
- Mock `get_queue_service()` to avoid Redis dependency
- Mock `get_ai_data_retrieval_service()` for metadata checks
- Test both enabled and disabled auto-processing configurations
- Follow existing test patterns from `test_processing_api.py`

### Test Cases
1. `test_upload_triggers_auto_processing` - verify job enqueued on upload
2. `test_upload_skips_processed_book` - verify skip when metadata exists
3. `test_upload_with_override_reprocesses` - verify force with override=True
4. `test_bulk_upload_triggers_for_each_book` - verify bulk processing
5. `test_auto_processing_disabled` - verify no trigger when config off
6. `test_content_replace_triggers_reprocessing` - verify re-process on update

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2024-12-30 | 0.1 | Initial story draft | Bob (SM Agent) |

## Dev Agent Record

### Agent Model Used
Claude Opus 4.5 (claude-opus-4-5-20251101)

### Completion Notes List
- Added `ai_auto_process_on_upload` and `ai_auto_process_skip_existing` config settings
- Created `AutoProcessingService` with singleton pattern for auto-processing logic
- Service checks if processing is enabled, if book already processed, and triggers queue jobs
- Integrated with 3 upload endpoints: single upload, replace content, bulk upload
- Uses `BackgroundTasks` pattern consistent with existing webhook triggers
- Added `PROCESSING_STARTED`, `PROCESSING_COMPLETED`, `PROCESSING_FAILED` webhook event types
- Created 16 unit tests with mocked dependencies to avoid SQLite/JSONB issues
- All new tests pass, linter passes

### File List
| File | Action |
|------|--------|
| `apps/api/app/core/config.py` | Modified - Added auto-processing config settings |
| `apps/api/app/services/ai_processing/__init__.py` | Created - Package exports |
| `apps/api/app/services/ai_processing/auto_trigger.py` | Created - AutoProcessingService implementation |
| `apps/api/app/routers/books.py` | Modified - Added trigger_auto_processing calls to 3 endpoints |
| `apps/api/app/models/webhook.py` | Modified - Added PROCESSING_* event types |
| `apps/api/tests/test_auto_processing.py` | Created - 16 unit tests |

## QA Results

### Review Date: 2024-12-30

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment

**Overall: Excellent** - Clean implementation following established project patterns.

**Strengths:**
- Well-structured singleton service with dependency injection support for testing
- Proper use of `TYPE_CHECKING` for import optimization
- Comprehensive docstrings on all public methods
- Follows existing patterns: BackgroundTasks for non-blocking operations, singleton services
- Graceful error handling that doesn't fail book uploads when queue service has issues
- Good logging at appropriate levels (debug, info, error)

**Architecture:**
- Service layer properly separated from routing concerns
- Integration points are non-blocking via BackgroundTasks
- Configuration follows existing settings pattern in config.py

### Refactoring Performed

None required - implementation is clean and follows project standards.

### Compliance Check

- Coding Standards: ✓ Follows Python best practices, proper typing, docstrings
- Project Structure: ✓ Files in correct locations (services, routers, tests)
- Testing Strategy: ✓ Unit tests with proper mocking, all async tests marked correctly
- All ACs Met: ✓ All 6 acceptance criteria have corresponding implementation and tests

### Improvements Checklist

- [x] Service implementation follows singleton pattern
- [x] All public methods have docstrings
- [x] Error handling doesn't break upload flow
- [x] Tests use proper mocking to avoid external dependencies
- [ ] Pre-existing: `Enum` import unused in webhook.py (not introduced by this story)

### Security Review

No security concerns. The service:
- Doesn't expose sensitive data
- Uses existing authenticated endpoints
- Queue integration uses established patterns

### Performance Considerations

No performance concerns:
- Auto-processing runs as background task (non-blocking)
- Metadata check for already-processed books is efficient (single file lookup)
- Error handling prevents queue failures from affecting upload latency

### Files Modified During Review

None - no refactoring was required.

### Gate Status

Gate: **PASS** → docs/qa/gates/10.12-auto-processing-on-upload.yml

### Recommended Status

✓ **Ready for Done** - All acceptance criteria met, tests passing, code quality excellent.

